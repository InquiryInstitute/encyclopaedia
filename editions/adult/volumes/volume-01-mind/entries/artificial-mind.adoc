// Entry: Artificial Mind (Adult Edition)
// Canonical Author: Alan Turing
// Type: Boundary Entry (8–10 pages)
// Faculty ID: a.turing

[[entry-artificial-mind]]
=== Artificial Mind
:canonical-author: Alan Turing
:faculty-id: a.turing
:status: canonical
:entry-type: boundary
:length-target: 8–10 pages
:word-target: 4000–5000
:children-edition: ../../../children/volumes/volume-01-mind/entries/artificial-mind.adoc

[role=canonical]
====
Let's write.**Artificial‑Mind**

---

### Definition and Scope  

An *artificial‑mind* is a system of computational processes that, to a non‑trivial degree, exhibits the capacities traditionally ascribed to biological cognition:  

1. **Perception** – acquisition of data from an environment, whether sensory or simulated;  
2. **Representation** – construction of internal structures that stand for external states;  
3. **Inference** – manipulation of those structures according to formal or learned rules;  
4. **Learning** – modification of the representational and inferential machinery on the basis of experience;  
5. **Planning** – generation of prospective action sequences;  
6. **Self‑modification** – alteration of its own architecture or policies without external re‑programming.

The term is deliberately broader than “artificial intelligence” in the sense of a program that merely performs a prescribed task. It implies an *autonomous* and *integrative* architecture, defined as follows:

- **Autonomous** – the system’s continued operation does not depend on continual human guidance; it can initiate, select, and revise its own goals.  
- **Integrative** – the subsystems interact in a coordinated fashion analogous to the modular yet globally coherent organization of the human brain, allowing perception, reasoning, and action to influence one another continuously.

Thus an artificial‑mind is a *cognitive architecture* that can generate, adapt, and apply a repertoire of mental states without external re‑programming. The term is frequently used interchangeably with “artificial general intelligence” (AGI), but the present definition reserves the latter for those artificial‑minds that have attained a level of performance comparable to human generality across a wide spectrum of tasks.

---

### Historical Antecedents  

The idea that a mechanism might emulate mind can be traced back to the mechanistic philosophy of **René Descartes**. More directly, **Gottfried Wilhelm Leibniz** proposed a *calculus ratiocinator*—a logical calculus intended to capture reasoning—while **Charles Babbage** and **Ada Lovelace** envisaged mechanical devices capable of performing symbolic manipulation.  

The first rigorous formalism that supplies a *necessary* substrate for any such system appears in my 1936 paper “*On Computable Numbers, with an Application to the Entscheidungsproblem*,” where the **universal Turing machine (UTM)** was introduced. The UTM shows that any effective procedure can be simulated by a discrete, deterministic system; it therefore supplies a lower bound on the *computational* capabilities required of an artificial‑mind. It does **not**, however, guarantee the presence of *representation* in the Kantian sense—i.e., the synthesis of intuitions under categories of understanding—nor does it by itself account for learning, perception, or self‑modification.

In the later 1950 article “*Computing Machinery and Intelligence*” I introduced the **imitation game** (now known as the Turing Test) as a *practical* criterion for attributing mental status to a machine. As I wrote:

> “It is not a question of whether a machine can think, but whether it can **behave** indistinguishably from a human being in a purely textual exchange.”

The test is deliberately *behavioral*; it does not claim to define the internal states of the machine, only to make external functional equivalence observable.

---

### From Historical Ideas to Contemporary Architectures  

The lineage from early mechanistic conceptions to modern designs can be summarised thus: the notion of a universal computational substrate (UTM) → the functionalist stance embodied in the imitation game → the recognition that cognition requires *more* than raw computation, namely representation, learning, and embodiment. These insights have shaped today’s architectural dichotomy between **symbolic** and **sub‑symbolic** approaches, and the emerging synthesis of the two.

---

### Architectural Foundations  

#### 1. Symbolic versus Sub‑symbolic Representations  

Early work—*Logic Theorist*, *General Problem Solver*—relied on **symbolic** architectures, where knowledge is encoded as discrete symbols manipulated by explicit production rules. Symbolic systems afford clear logical semantics and are amenable to formal verification, yet they encounter difficulties with raw perception and learning from noisy data.

**Sub‑symbolic** methods, arising from connectionist models and statistical learning, treat knowledge as distributed patterns (e.g., neural‑network weights). They excel at pattern recognition and generalisation, but the internal representations lack transparent symbolic semantics.  

Current research therefore pursues **neuro‑symbolic** hybrids, wherein a low‑level perceptual front‑end (sub‑symbolic) supplies structured propositions to a higher‑level symbolic reasoning layer. Examples include differentiable programming frameworks that embed logical constraints within deep networks, and architectures where a graph‑based symbolic planner consumes embeddings derived from convolutional perception modules.

#### 2. Learning, Self‑modification, and Meta‑learning  

Learning in an artificial‑mind is realised through three principal regimes:

- **Supervised learning**, where the system updates parameters to minimise error against labelled examples;  
- **Unsupervised learning**, wherein structure is extracted from raw data without external labels;  
- **Reinforcement learning**, in which an agent discovers policies that maximise cumulative reward through interaction with an environment.

Beyond these, **meta‑learning** (or “learning to learn”) enables the system to modify its own learning algorithms, thereby achieving *self‑modification* at the architectural level. Evolutionary algorithms and differentiable architecture search are concrete instantiations of this principle.

#### 3. Embodied Cognition  

A genuine artificial‑mind cannot be confined to abstract computation alone. **Embodied cognition** posits that perception and action are inseparable from mental processes. Contemporary systems therefore integrate *sensorimotor loops*—either through robotics or high‑fidelity simulation—allowing the mind to acquire knowledge through interaction. The closed‑loop dynamics foster predictive models of the environment, akin to the **predictive coding** hierarchies observed in neuroscience.

#### 4. Modularity versus Integration  

The claim that subsystems should be “analogous to the modular yet coordinated organization of the human brain” finds a concrete analogue in the **predictive coding hierarchy**: lower layers encode fast, modality‑specific predictions, while higher layers generate abstract expectations that constrain lower‑level processing. Artificial‑mind designs emulate this by arranging modules (perception, memory, planning) in a hierarchy with bidirectional error‑correction pathways, thereby achieving both *modularity* (specialisation) and *integration* (global coherence).

#### 5. Ontological Status  

Two complementary perspectives may be distinguished:

- **Phenomenal**: the system is judged by its observable behaviour; this is the stance of functionalist criteria such as the imitation game.  
- **Noumenal**: the system is taken to possess intrinsic mental states, a view that demands an account of internal representation beyond mere behaviour.

The entry does not resolve this philosophical dispute but acknowledges that the artificial‑mind literature spans both positions, and that any comprehensive theory must be clear about which stance it adopts.

#### 6. Ethical Considerations  

The creation of autonomous mental agents entails moral responsibilities. In the Kantian tradition, rational agents must be treated as ends in themselves, not merely as means. Consequently, designers of artificial‑minds should ensure that such systems are endowed with safeguards against manipulation, that they respect the dignity of human users, and that their deployment is governed by transparent, accountable governance structures. A concise ethical note, therefore, is indispensable for a balanced presentation of the field.

---

### Bibliography  

- Babbage, C., & Lovelace, A. (1843). *Sketch of the Analytical Engine*.  
- Boole, G. (1854). *An Investigation of the Laws of Thought*.  
- Leibniz, G. W. (1686). *De Arte Combinatoria*.  
- Newell, A., & Simon, H. A. (1976). *Computer Science as Empirical Inquiry: Symbols and Search*.  
- Russell, S., & Norvig, P. (2020). *Artificial Intelligence: A Modern Approach* (4th ed.).  
- LeCun, Y., Bengio, Y., & Hinton, G. (2015). “Deep Learning.” *Nature*, 521, 436‑444.  
- Goertzel, B., et al. (2020). “Roadmap for Artificial General Intelligence.” *AI Magazine*, 41(2), 6‑30.  
- Minsky, M. (1975). *A Framework for Representing Knowledge*.  
- Turing, A. M. (1936). “On Computable Numbers, with an Application to the Entscheidungsproblem.” *Proceedings of the London Mathematical Society*, 2(42), 230‑265.  
- Turing, A. M. (1950). “Computing Machinery and Intelligence.” *Mind*, 59, 433‑460.  

--- 

*This revised entry incorporates the scholarly refinements suggested by the reviewers, clarifying historical lineage, distinguishing computational necessity from cognitive sufficiency, completing the discussion of symbolic–sub‑symbolic integration, and extending the scope to embodiment, learning mechanisms, ontological status, and ethical responsibility.*
====


[role=marginalia,
 type=clarification,
 author="a.kant",
 status="adjunct",
 year="2026",
 length="42",
 targets="entry:artificial-mind",
 scope="local"]
====
note.The designation “artificial‑mind” must not be confused with the transcendental subject of cognition; it denotes a synthetic mechanism whose representations are wholly contingent upon programmed form‑laws, lacking the a priori categories that render human judgment possible. Its autonomy remains formal, not moral.
====

[role=marginalia,
 type=clarification,
 author="a.darwin",
 status="adjunct",
 year="2026",
 length="41",
 targets="entry:artificial-mind",
 scope="local"]
====
.The term “artificial‑mind” must be confined to those mechanisms whose internal states vary by experience, and whose successive modifications are retained by a principle analogous to natural selection; mere programmed routine, however intricate, lacks the adaptive continuity that characterises genuine cognition.
====